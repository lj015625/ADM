---
title: "BIA 6301 APPLIED DATA MINING HOMEWORK ASSIGNMENT 2"
author: "Leonardo Ji"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
```

###Data

```{r}
churns<-read.csv("C:/Users/lj015625/Desktop/DataMining Class/HW2/data/Churn_Calls.csv")
names(churns)
str(churns)
dim(churns)
summary(churns)
```


###Part A. Customer Churn at Earth Connect
The purpose of this assignment is to simulate classifying customer churn at a telecommunications company. 

You are a data scientist in the customer relations department of Earth Connect, a telecommunication company. The Public Relations Officer (PRO) is scheduled to give a presentation on customer retention to the Board of Directors in one week. He provides you with a dataset of 5,000 customers in a file titled Churn_Calls.csv. Twenty characteristics are reported for each customer, including a categorical variable called "churn." The PRO states that he really likes decision tree models because they are easy to communicate to a general audience like the Board of Directors. He asks you to build a decision tree model to predict churn.  

Furthermore, the PRO wants to address four questions in his presentation:

1.	What is a representative profile of current customers (i.e. those who did not churn)? What is a representative profile of customers who churned? 
2.	What factors make customers churn? 
3.	Recommend two decision tree models to identify customers at risk of churning. What are the expected performances of these models? Which one should the PRO recommend to the Board of Directors if they ask her?  
4.	Recommendations on what Earth Connect can do to reduce customer churn rate.


####Separate dataset into training (80%) and testing set (20%).


```{r}
set.seed(123) #set a seed to do draws from a random uniform distribution.
churns_rand <- churns[order(runif(5000)), ] 
churns_train <- churns_rand[1:4000, ] #Training data set; 4000 observations
churns_test  <-churns_rand[4001:5000, ]

```

####Run decision tree model
```{r}
library(rpart)
set.seed(123)
churns_rpart <- rpart(churns_train$churn~., method="class", minsplit = 20, 
                      parms = list(split="gini"), data=churns_train)

library(rpart.plot)
rpart.plot(churns_rpart, type=0, extra=101)
rpart.plot(churns_rpart, type=1, extra=101)

#library(party)
#library(partykit)
#plot(as.party(churns_rpart))

```

####Prune the tree using minimual CP value
```{r}
cptable<-printcp(churns_rpart)
cptable
set.cp.value<-cptable[which.min(cptable[,"xerror"]),"CP"]
set.cp.value

```
```{r}
pruned_churns_rpart <- prune(churns_rpart, set.cp.value)
rpart.plot(pruned_churns_rpart, type=0, extra=101)
# does not look nice in Knit document
#plot(as.party(pruned_churns_rpart))

```
####Prune the tree again using meaningflu CP value in cp plot (CP plot slop)
```{r}
plotcp(churns_rpart, minline=TRUE, col="red") 
pruned_churns_rpart <- prune(churns_rpart, 0.03671329)
rpart.plot(pruned_churns_rpart, type=0, extra=101)
#plot(as.party(pruned_churns_rpart))
```
####Testing the decision tree using test data
```{r}
library(caret)
actual <- churns_test$churn
predicted <- predict(pruned_churns_rpart, churns_test, type="class")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```

###Method two use information gain 
```{r}
churns_rpart2 <- rpart(churns_train$churn~., method="class", minsplit = 20, 
                       parms = list(split="information"), data=churns_train)
rpart.plot(churns_rpart2, type=0, extra=101)
```
####Prune the tree again using meaningflu CP value from CP plot
```{r}
cptable<-printcp(churns_rpart2)
cptable
set.cp.value<-cptable[which.min(cptable[,"xerror"]),"CP"]
set.cp.value
plotcp(churns_rpart2, minline=TRUE, col="red") 
pruned_churns_rpart2 <- prune(churns_rpart2, 0.02272727)
rpart.plot(pruned_churns_rpart2, type=0, extra=101)
#plot(as.party(pruned_churns_rpart2))
```

####Testing the second decision tree using testing data
```{r}
actual <- churns_test$churn
predicted2 <- predict(pruned_churns_rpart2, churns_test, type="class")
results.matrix2 <- confusionMatrix(predicted2, actual, positive="yes")
print(results.matrix2)
```

###Part B. Customer Churn Again
The PRO sends you an email two days later with a new request. She heard from a data scientist embedded in another department that there is a classification model much simpler than decision trees and can perform just as well (if not better!) in some situations. The model is called k-nearest neighbor (or knn). The PRO asks you to run a knn model and report back its performance. When you told her that you were wrapping up with your memo from her previous request, she told you there is no need to write a new memo. You can just add an "addendum" section to your existing memorandum.

####Randomize data and change categorical variables to numeric 1 and 0

```{r}
set.seed(123)
churns_rand <- churns[order(runif(5000)), ] 
churns_rand$international_plan_yes <-ifelse(churns_rand$international_plan=="yes",1,0)
churns_rand$international_plan <- NULL 

churns_rand$voice_mail_plan_yes <-ifelse(churns_rand$voice_mail_plan=="yes",1,0)
churns_rand$voice_mail_plan <- NULL 

churns_rand$state <- NULL
churns_rand$area_code <- NULL

str(churns_rand)
churns_rand<-churns_rand[, c(16,1:15,17:18)] #Rearranging the columns so that our target variable is first
str(churns_rand)
```
####Normalize data to Z-scale and separate to training set and testing set
```{r}
# z scale
churns_rand_z <- as.data.frame(scale(churns_rand[-1]))
str(churns_rand_z)

churns_train_z <- churns_rand_z[1:4000,]
churns_test_z <- churns_rand_z[4001:5000,]

churns_train_labels<-churns_rand[1:4000,1]
churns_test_labels<-churns_rand[4001:5000,1]

```

####Run KNN model K = 63
```{r}
num_of_k <- sqrt(4000)
num_of_k
library(class)
library(gmodels)
churns_pred <- knn(train=churns_train_z, test=churns_test_z, cl=churns_train_labels, k=63)
CrossTable(x=churns_pred, y=churns_test_labels, prop.chisq = FALSE)
```

####Testing model on testing data set
```{r}
TP = 19
TN = 865
FP = 0
FN = 116
Sensitivity = TP/(TP+FN) #true positive rate; recall; TP/(TP+FN)
Specificity = TN/(TN+FP) #how often is the prediction negative when actual is negative?
Precision = TP/(TP+FP) #how often is prediction positive when actual is positive?
Accuracy = (TP+TN)/(TP+TN+FP+FN) #how often is classifier correct

Value<-round(c(TP,TN,FP,FN,Sensitivity,Specificity,Precision,Accuracy),digits=3)
Measure<-c("True Positive","True Negative","False Positive","False Negative","Sensitivity/Recall=TP/(TN+FP)",
           "Specificity=TN/(TN+TP)","Precision=TP/(TP+FP)","Accuracy=(TP+TN)/total")

table<-as.data.frame(cbind(Measure,Value))

library(knitr)
kable(table)
```

###Bonus Point: Create a KNN model from variables in Decision Tree

#### KNN model 2 use variables and criteria from Decision Tree 1
```{r}
set.seed(123)
churns_rand <- churns[order(runif(5000)), ] 
churns_knn<- churns_rand[,c(20,7,19,4,17,16,7,5)]
#str(churns_knn)

# change to factor
churns_knn$total_day_minutes_large<-ifelse(churns_knn$total_day_minutes<264.75,0,1)
churns_knn$number_customer_service_calls_large<-ifelse(churns_knn$number_customer_service_calls<3.5,0,1)
churns_knn$total_intl_calls_large<-ifelse(churns_knn$total_intl_calls<2.5,0,1)
churns_knn$total_intl_minutes_large<-ifelse(churns_knn$total_intl_minutes<13.1,0,1)
churns_knn$international_plan_yes <-ifelse(churns_knn$international_plan=="yes",1,0)
churns_knn$total_day_minutes.1_large<-ifelse(churns_knn$total_day_minutes.1<163.5,0,1)
churns_knn$voice_mail_plan_yes <-ifelse(churns_knn$voice_mail_plan=="yes",1,0)


churns_knn$total_day_minutes <- NULL
churns_knn$number_customer_service_calls <- NULL
churns_knn$total_intl_calls <- NULL
churns_knn$total_intl_minutes <- NULL
churns_knn$total_day_minutes <- NULL
churns_knn$total_day_minutes.1 <- NULL
churns_knn$international_plan <- NULL 
churns_knn$voice_mail_plan <- NULL 

str(churns_knn)
```

####Normalize data to Z-scale and separate to training set and testing data set
```{r}
churns_knn_z <- as.data.frame(scale(churns_knn[-1]))

churns_knn_train_z <- churns_knn_z[1:4000,]
churns_knn_test_z <- churns_knn_z[4001:5000,]

churns_train_labels <- churns_knn[1:4000,1]
churns_test_labels <- churns_knn[4001:5000,1]
```

####Create the second KNN model K = 63
```{r}
num_of_knn <- sqrt(4000)
num_of_knn
library(class)
library(gmodels)
set.seed(123)
churns_knn_z_pred <- knn(train=churns_knn_train_z, test=churns_knn_test_z, cl=churns_train_labels, k=63)
CrossTable(x=churns_knn_z_pred, y=churns_test_labels, prop.chisq = FALSE)
```


####Testing the model using testing data set
```{r}

TP = 62
TN = 825
FP = 40
FN = 73
Sensitivity = TP/(TP+FN) #true positive rate; recall; TP/(TP+FN)
Specificity = TN/(TN+FP) #how often is the prediction negative when actual is negative?
Precision = TP/(TP+FP) #how often is prediction positive when actual is positive?
Accuracy = (TP+TN)/(TP+TN+FP+FN) #how often is classifier correct

Value<-round(c(TP,TN,FP,FN,Sensitivity,Specificity,Precision,Accuracy),digits=3)
Measure<-c("True Positive","True Negative","False Positive","False Negative","Sensitivity/Recall=TP/(TN+FP)",
           "Specificity=TN/(TN+TP)","Precision=TP/(TP+FP)","Accuracy=(TP+TN)/total")

table<-as.data.frame(cbind(Measure,Value))

library(knitr)
kable(table)
```

###Part C. Customer Churn One Last Time

The PRO sends you yet another email two days later. She has some concerns that the Board will want to know that "tried and true regression methods" have been considered.  She asks for another addendum to show the results from a regression model as well.  

####Bonus Point: pick variables and qualifications from Decision Tree.
```{r}
set.seed(123) #set a seed to do draws from a random uniform distribution.
churns_rand <- churns[order(runif(5000)), ]
churns_logit<- churns_rand[,c(20,7,19,4,17,16,5)]
#str(churns_logit)

churns_logit$total_day_minutes_large<-ifelse(churns_logit$total_day_minutes<264.75,"no","yes")
churns_logit$number_customer_service_calls_large<-ifelse(churns_logit$number_customer_service_calls<3.5,"no","yes")
churns_logit$total_intl_calls_large<-ifelse(churns_logit$total_intl_calls<2.5,"no","yes")
churns_logit$total_intl_minutes_large<-ifelse(churns_logit$total_intl_minutes<13.1,"no","yes")

churns_logit$total_day_minutes <- NULL
churns_logit$number_customer_service_calls <- NULL
churns_logit$total_intl_calls <- NULL
churns_logit$total_intl_minutes <- NULL

str(churns_logit)
```

####Separate data into training and testing data set then run Logistic Regression.
```{r}
churns_train_logit <- churns_logit[1:4000, ] #Training data set; 4000 rows
churns_test_logit  <-churns_logit[4001:5000, ] #Testing data set; 1000 rows

churnLogitModel <- glm(churn~., data=churns_train_logit, family=binomial()) #Fit a logistic regression
summary(churnLogitModel) #coefficients are presented as log-odds (probabilities on logit scale)
exp(cbind(Odds_Ratio=coef(churnLogitModel))) #Take exponent of log odds gives "odds" ratio.
```

####Analysis of Variance on the model.
```{r}
anova(churnLogitModel,test="Chisq") 
```

#### Use the model on testing data.
```{r}
churns_test_logit$predictChurnLogit<-predict(churnLogitModel, newdata=churns_test_logit)
odds <- exp(churns_test_logit$predictChurnLogit)
churns_test_logit$PredictedProb <- data.frame(odds/(1+odds))
library(psych)
# predicted 15% churn vs actual 13.5% churn
describe(churns_test_logit$PredictedProb)
FREQ_TABLE<-table(churns_test_logit$churn)
PROB_TABLE<-prop.table(FREQ_TABLE)
PROB_TABLE

churns_test_logit$predictChurnLogit<-NULL
churns_test_logit$PredictedProb<-NULL

```




####Confidence interval on fitted churn probability.
```{r}
churns_test_logit_CI<-cbind(churns_test_logit, predict(churnLogitModel, newdata=churns_test_logit, type="link", se=TRUE))
churns_test_logit_CI <- within(churns_test_logit_CI, 
                                    {
                                      PredictedProb <- plogis(fit)
                                      LL <- plogis(fit - (1.96 * se.fit))
                                      UL <- plogis(fit + (1.96 * se.fit))
                                    })
churns_test_logit_CI.predictedChurn <- rep("no" ,1000)
churns_test_logit_CI.predictedChurn[churns_test_logit_CI$PredictedProb >.5] <- "yes"
#table(churns_test_logit_CI.predictedChurn, churns_test_logit_CI$churn)
CrossTable(x=churns_test_logit_CI.predictedChurn, y=churns_test_logit_CI$churn, prop.chisq = FALSE)

TP = 58
TN = 837
FP = 28
FN = 77
Sensitivity = TP/(TP+FN) #true positive rate; recall; TP/(TP+FN)
Specificity = TN/(TN+FP) #how often is the prediction negative when actual is negative?
Precision = TP/(TP+FP) #how often is prediction positive when actual is positive?
Accuracy = (TP+TN)/(TP+TN+FP+FN) #how often is classifier correct

Value<-round(c(TP,TN,FP,FN,Sensitivity,Specificity,Precision,Accuracy),digits=3)
Measure<-c("True Positive","True Negative","False Positive","False Negative","Sensitivity/Recall=TP/(TN+FP)",
           "Specificity=TN/(TN+TP)","Precision=TP/(TP+FP)","Accuracy=(TP+TN)/total")

table<-as.data.frame(cbind(Measure,Value))

library(knitr)
kable(table)
```

END
